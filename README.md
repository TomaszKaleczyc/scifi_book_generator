

<img src="data/cover.jpg" height=300></img><img src="data/cover2.jpg" height=300></img><img src="data/cover3.jpg" height=300></img>
# SciFi book generator

The purpose of this project is to build a decoder only transformer architecture from scratch and train it to autogenerate SciFi stories based on transcripts from pulp magazines. The approach taken in the code is inspired by the original [Attention is all you need](https://arxiv.org/abs/1706.03762) as well as [Andrej Karpathy's coding implementation](https://www.youtube.com/watch?v=kCc8FmEb1nY)

## Resources

* Working environment pre-requisites: Ubuntu18.04 LTS / Python 3.6.9 / unzip / virtualenv / CUDA version >=11.6
* Dataset: [Kaggle SciFi Stories Text Corpus](https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus?resource=download) - use: `make download-dataset` to collect


## Project structure 

```
├── data                             # project source data
├── environment                      # project dependencies 
├── output                           # stored PyTorch Lightning output of training
└── src                              # project source code
    ├── dataset                      # dataset creation tools
    ├── model                        # model definitions
    |     └── transformer_modules    # transformer element definitions
    └── tokeniser                    # text tokenisers
```

## Makefile

The project [Makefile](Makefile) allows performing the following tasks by typing the following commands in terminal from the project root:

* `create-env` - build virtual environment to run the project code
* `activate-env-command` - generate command to activate the project virtual environment
* `download-dataset` - download the Kaggle dataset (requires Kaggle account and token)
* `run-training` - run training based on the contents of [train.py](src/train.py)
* `purge-output` - deletes all output generated by PyTorch Lightning after the training
* `run-tensorboard` - creates link to tensorboard with training results

## Project overview & findings

### Data

The following tokenisers have been implemented for use:
* character tokenisation
* sub-words by [tiktoken](https://github.com/openai/tiktoken) 

Given the limited offline resources (GTX 1060 with 6GB VRAM), we resorted to using the character tokenisation for the purpose of this analysis.

### Training

Training was performed on the following architectures:
* [bigram (baseline)](src/model/bigram_model.py)
* [single head transformer](src/model/single_head_transformer.py)
* [multi head transformer (single stack)](src/model/multi_head_transformer.py)
* [final multi head transformer (multiple stacks)](src/model/final_multi_head_transformer.py)

Models were trained on the typical language modelling task of autocompleting a sequence of tokens. To allow fair comparison, all models were trained on approximately 5k batches and evaluated on 500 holdout batches using cross entropy loss. Where possible, the same hyperparameters were used in all approaches to allow more fair comparison (see [config](src/config.py))

The result overview from tensorboard can be seen below:

<img src="data/loss.jpg">


**Bigram model**

As baseline a bigram model was trained with 5.6K trainable parameters, reaching the validation loss of 2.45. Given the naive approach (basically predicting the next character given a lookup table based only on the current character), the model output is little more than pure giberish:

> ujC; d ashepea thisand. lyoThengowabosiofazer. hePan ave. ane scu d,he pll atinr tete thomowanogl.V. Yogasow,"I Bantrelsisn miphex ng t58Pl !quowin ) s, whe tho6gulldiUn d. venoomere histitheang. J. butrerexed ged He coom mang wheerly gxcQenlworouritheniUy w!ainongerind toblery pou!A af mbe se. th thilller aned cr ulis?" " wange Norv! f t lirgacote anewas me wnsinechot."Gr pld She the wabrighe f prrthittheay sn cin ps whexphie Ap vetMurein" we errototughis omole laredm: Cotsa bet mbo3Nofinot) d. 'tl lugrend."Hesharche?u ot1ze sppflefoul. oraimeywative. the Muntan wamemamer, sainewhelhe w seviond. in org r, Z, wa ave, d sabl rcu aing cang "Por L'doof I've c7any hending a ugn seresulaYe te sKar-Latoum "Anes. Ther. he m o h hined s cienotoupeacrty he. to woutanghiemorivie ut I4Ut "I gDomof ! b be wat, touguke, tJegio inrs commalod ancerevck yeak y an ake ornejorbyG78Lor here Kio lect44ugher ecquns and aRomeCabec2Lal Hat wiasthis elly ld edu masuo! g hatlale t7PkNoul ctangintherwhed wof The


**Single head transformer**

The next approach used was a very basic transformer implementation, using only a single self-attention head with 598K trainable parameters. This allowed the validation loss to drop to 2.28, with the following sample output:

> (  oirg oncthe apce imstyo kact heeve tis. doonwand, odupperon hipthe hankeripn. Hee nopothery stioveecenely onw he boustinereneretey Best tap adr bee wo to nkeand cildin an. Thergiforct his that hiames:mon f-n. Weher ieve ashy Don. Tr fork bo utre pesepll, pply ih pingge."Singunsed ss beenende aly wising hed'm cancung stedas an rins sal the ck...  tt bastveers's and latk, borne Sctim -veslolele s jipakemsoak for haly pet andi nol?  tm gutile woou alno wh ting sthe id. "I net oing tand on se hereculdd rald atr oft ig, dsenck younet f sill? Wof witho corrceds feverd hofurpocs, I be the frllamoudlllet. Atte iby woused. Wamlickse avanin teand epthe one joulllinto day ppereed Fs thre ow, for he se os nestanvee idziten ve hof upreird. Andcol. Deninglat am soniocer stoun wsser sero beto ss fioldid, al, too roftired lest rceler) verspeanwofd dnne. Unt to wicl or ringinnd, thamitheyyu sa nimauts mprk smia bork blisieclon, ave gopo biscin ske, tthirlet Thaade. "Dingeatt theeadn'l nedr, Coow thal


Progress compared to the bigram approach is minimal.


**Multi-head transformer (single stack)**

Next, a transformer with multiple self-attention heads (in a single decoder stack) was trained. With 1.9M trainable parameters we used a smaller learning rate to train the architecture. This architecture managed to achieve a validation loss of 1.59. This is an example random generated output from the network:

> 1mong as us turne-played in on matter arrial, word slowarmlears. I has to evitualizaras timod a was now it that of a murced in wonder as warf against to it with fron's row," Plearse," he time what sible, Righn too." New for on.. And robot," Brunned man mport owere not at of aving to do," he said, "I was mai t lice sual veleving pement on seezed a your composs fored wered through momes on said. His such that your field of find the baracks, "You're takes own so ave -- anyway from Himbad the you or comptionion. Mybe. Pl he in dorankfashed a but the did a a but it, it Claution on I have insultants." The Enory F Cluff desters, forticreten fly now up grave ubt ourse. I shappred tilt ened the prote haved that's a crace abought doesn't been trifice, the last Igened the bask, but to show halve back cate deved as so a homput that aliy? She sadvant was it?" let thered bulb. We hop-so be the resoltern roble new we was alone any enew skiruter is por-ped in Coxident blurium?" All I've nly reg. What t

This time around we can clearly see actual english words emerging from the output, although there's still a number of incorrect wordlike sequences.

**Final multi-head transformer**

Lastly, a multi-head transformer with multiple decoder stacks was used. This pushed the number of trainable parameters to 10.8M, increasing the training time accordingly compared to previous approaches. Again, a smaller learning rate was used given the network depth. The final validation loss achieved was 1.34, with signs of possible further improvement. 

Example generated output:

> ch, improbably. All I gave the young." His byth dialty near Edithbon was relying step on the world -- and a leathing for the other remoments, and he long me sod. "I have insurprised when he left the door than with a bit death." Verjice more said with the newchnthrough Running his head difficulty metal and flesh signs. "Imporable them my better attack?" Parkers torribly turned up to Her around, streaking of her body, toward, he gave a case entered cubic chaoscilence. He had slipped away a vehicle for succession three years and the short of life, leaves swinging here at the black and treatment, only trailing it on this massive shape, but would ever be called to do with armuda. "Sorry?" he clusted the day. "We don't not took Money to make all this company of nothing feed." "Those project. Our polite is no isn't about weeks," Retief said, "took your own." "It stood before we don't mean wait to keep about the field." That's the planet uncase the olde attendance from it?" "I'm goting to pract

This time around the output actually looks like lines from a book. Of course, there is little cohesion in the sentences themselves. However given the still relatively shallow nature of the architecture compared to gigantic architectures like GPT-3, this can be considered an impressive result.
